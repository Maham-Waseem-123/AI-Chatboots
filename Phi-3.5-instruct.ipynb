{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:49:14.096123Z","iopub.status.busy":"2024-09-29T10:49:14.095748Z","iopub.status.idle":"2024-09-29T10:49:32.715957Z","shell.execute_reply":"2024-09-29T10:49:32.714812Z","shell.execute_reply.started":"2024-09-29T10:49:14.096074Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting transformers_stream_generator\n","  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n","Collecting optimum\n","  Downloading optimum-1.22.0-py3-none-any.whl.metadata (20 kB)\n","Collecting auto-gptq\n","  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: transformers>=4.26.1 in /opt/conda/lib/python3.10/site-packages (from transformers_stream_generator) (4.44.2)\n","Collecting coloredlogs (from optimum)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.13.3)\n","Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.4.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\n","Requirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\n","Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.25.0)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (3.0.0)\n","Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.34.2)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\n","Collecting rouge (from auto-gptq)\n","  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n","Collecting gekko (from auto-gptq)\n","  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.4.5)\n","Collecting peft>=0.5.0 (from auto-gptq)\n","  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.66.4)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.19.1)\n","Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.45.0,>=4.29->optimum) (3.20.3)\n","Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.5)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading optimum-1.22.0-py3-none-any.whl (453 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.7/453.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: transformers_stream_generator\n","  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12425 sha256=d34aa39228f60629922442ce9a255c0d9aa3b7394c6935eb01e3c373625f46ef\n","  Stored in directory: /root/.cache/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\n","Successfully built transformers_stream_generator\n","Installing collected packages: rouge, humanfriendly, gekko, einops, tiktoken, coloredlogs, transformers_stream_generator, peft, optimum, auto-gptq\n","Successfully installed auto-gptq-0.7.1 coloredlogs-15.0.1 einops-0.8.0 gekko-1.2.1 humanfriendly-10.0 optimum-1.22.0 peft-0.13.0 rouge-1.0.1 tiktoken-0.7.0 transformers_stream_generator-0.0.5\n"]}],"source":["!pip install tiktoken transformers_stream_generator einops optimum  auto-gptq"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:49:37.854204Z","iopub.status.busy":"2024-09-29T10:49:37.853753Z","iopub.status.idle":"2024-09-29T10:49:49.357767Z","shell.execute_reply":"2024-09-29T10:49:49.356631Z","shell.execute_reply.started":"2024-09-29T10:49:37.854153Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n"]}],"source":["!pip install huggingface_hub"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:49:52.612828Z","iopub.status.busy":"2024-09-29T10:49:52.611865Z","iopub.status.idle":"2024-09-29T10:49:56.739471Z","shell.execute_reply":"2024-09-29T10:49:56.738536Z","shell.execute_reply.started":"2024-09-29T10:49:52.612782Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from transformers.generation import GenerationConfig\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import textwrap\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["Phi-3.5-instruct"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:54:03.195626Z","iopub.status.busy":"2024-09-29T10:54:03.195231Z","iopub.status.idle":"2024-09-29T10:54:26.157317Z","shell.execute_reply":"2024-09-29T10:54:26.156142Z","shell.execute_reply.started":"2024-09-29T10:54:03.195588Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","optimum 1.22.0 requires transformers[sentencepiece]<4.45.0,>=4.29, but you have transformers 4.45.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -qU transformers accelerate"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:54:47.071087Z","iopub.status.busy":"2024-09-29T10:54:47.070663Z","iopub.status.idle":"2024-09-29T10:55:35.302617Z","shell.execute_reply":"2024-09-29T10:55:35.301810Z","shell.execute_reply.started":"2024-09-29T10:54:47.071044Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5948540bd0845dcaf4e92ef33137065","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51074a3367b4471aae9d1cc472e27c4c","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9089103bb38b4c8da2c7544b609e9178","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24c83ce7b9d64c31b49b6855f5fd359e","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f291b67b7b624e78b9ce3a19c03cdd73","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2480e8f6b434d6d9448d62687117616","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13a142e8e3a54f1c83544afba3e9fa31","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b90ba84a62f4f6480fed2a87de75d4f","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8766ca395f324dae81f638d0e1eca183","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00009.safetensors:   0%|          | 0.00/998M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c26e0a16eedf403ebadcc44993a5cb5e","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00009.safetensors:   0%|          | 0.00/982M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea7739689621426c94a3cbb121e6720a","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00009.safetensors:   0%|          | 0.00/956M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e66bfd7c9a7b40999c5f26fd0aa65834","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00009.safetensors:   0%|          | 0.00/906M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbec292f058c4cebb0b23b41f071f84b","version_major":2,"version_minor":0},"text/plain":["model-00005-of-00009.safetensors:   0%|          | 0.00/906M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3594cc3f5a714c70a7114e12d52eb555","version_major":2,"version_minor":0},"text/plain":["model-00006-of-00009.safetensors:   0%|          | 0.00/906M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21b31f7c373c414a8296b0672b607f5f","version_major":2,"version_minor":0},"text/plain":["model-00007-of-00009.safetensors:   0%|          | 0.00/906M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30ddf928051c4db594b937ebbb4ace07","version_major":2,"version_minor":0},"text/plain":["model-00008-of-00009.safetensors:   0%|          | 0.00/906M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bdc186b667454bc689c89e4b2801a3b9","version_major":2,"version_minor":0},"text/plain":["model-00009-of-00009.safetensors:   0%|          | 0.00/176M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e16a8013e604906bf66fbe360636925","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","# Loading\n","tokenizer = AutoTokenizer.from_pretrained(\"MrOvkill/Phi-3-Instruct-Bloated\")\n","model = AutoModelForCausalLM.from_pretrained(\"MrOvkill/Phi-3-Instruct-Bloated\")\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:55:44.119834Z","iopub.status.busy":"2024-09-29T10:55:44.118386Z","iopub.status.idle":"2024-09-29T10:55:44.125992Z","shell.execute_reply":"2024-09-29T10:55:44.124763Z","shell.execute_reply.started":"2024-09-29T10:55:44.119773Z"},"trusted":true},"outputs":[],"source":["email = '''\n","Dear WidgetCorp,\n","I am writing to express my deep disappointment with your service regarding my recent order (Order\n","#98765). I had ordered the \"SuperWidget\" as a birthday gift for my daughter, Emily, whose birthday\n","was on March 15th. Despite your website promising delivery within three days, the product arrived\n","only yesterday, missing her birthday entirely.\n","I attempted to contact your customer service multiple times for an explanation and to expedite the\n","delivery, but to no avail. My calls and emails went unanswered, leaving me in a difficult position with\n","no gift for Emily’s special day.\n","This experience has left a sour taste, and I am reconsidering future purchases from WidgetCorp. I\n","expect a prompt explanation and an assurance that such delays will not occur in the future.\n","Sincerely,\n","Jane Doe\n","'''"]},{"cell_type":"markdown","metadata":{},"source":["Summarization"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:56:19.277497Z","iopub.status.busy":"2024-09-29T10:56:19.277072Z","iopub.status.idle":"2024-09-29T10:58:09.537737Z","shell.execute_reply":"2024-09-29T10:58:09.536002Z","shell.execute_reply.started":"2024-09-29T10:56:19.277460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["```\n","Dear WidgetCorp,\n","I am writing to express my deep disappointment with your service regarding my recent order (Order\n","#98765). I had ordered the \"SuperWidget\" as a birthday gift for my daughter, Emily, whose birthday\n","was on March 15th. Despite your website promising delivery within three days, the product arrived\n","only yesterday, missing her birthday entirely.\n","I attempted to contact your customer service multiple times for an explanation and to expedite the\n","delivery, but to no avail. My calls and emails went unanswered, leaving me in a difficult position with\n","no gift for Emily’s special day.\n","This experience has left a sour taste, and I am reconsidering future purchases from WidgetCorp. I\n","expect a prompt explanation and an assurance that such delays will not occur in the future.\n","Sincerely,\n","Jane Doe\n","```\n","\n","Solution 1:\n","Dear WidgetCorp,\n","\n","Jane Doe expresses deep disappointment with the delayed delivery of her \"SuperWidget\" order #98765, which missed her daughter Emily's birthday. She faced unresponsive customer service and now reconsiders future purchases from your company.\n","\n","\n","Instruction 2 (More Difficult):\n"," Analyze the provided excerpt from a research paper on climate change, focusing on the methodology used for data collection and the implications of the findings for future policy decisions. Ensure your analysis is concise, no more than 200 words, and includes at least two scholar\n"]}],"source":["#Prepare the prompt for summarization\n","prompt = f\"\"\"\n","Summarize the following email delimited by triple backticks in 30 words.\n","\n","Email:\n","```{email}```\n","\"\"\"\n","\n","# Tokenize the input prompt\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","\n","# Generate the summary with the model\n","outputs = model.generate(**inputs, max_new_tokens=150)\n","\n","# Decode the generated summary\n","summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# Extract only the summary part by removing the prompt text\n","summary = summary.split('Email:')[1].strip()\n","\n","print(summary)"]},{"cell_type":"markdown","metadata":{},"source":["Question/Answers"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:58:17.335796Z","iopub.status.busy":"2024-09-29T10:58:17.335066Z","iopub.status.idle":"2024-09-29T10:58:31.087490Z","shell.execute_reply":"2024-09-29T10:58:31.086428Z","shell.execute_reply.started":"2024-09-29T10:58:17.335741Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1. What was the reason for the customer's disappointment with WidgetCorp? 2. How\n","did WidgetCorp's customer service respond to the customer's inquiries about the\n","late delivery?  Please answer only the questions, without any additional context\n","or the email text.  Answers: 1. The customer was disappointed because the\n","\"SuperWidget\" ordered as a birthday gift for her daughter arrived late, missing\n","the daughter's birthday celebration. 2. WidgetCorp's customer service did not\n","respond to the customer's inquiries about the late delivery. 1. The customer's\n","disappointment stemmed from the late arrival of the \"SuperWidget,\" which was\n","intended as a birthday gift for her daughter, causing the gift to miss the\n","daughter's birthday celebration. 2. WidgetCorp's customer service did not\n","provide a response to the customer's attempts to inquire about the late\n","delivery.\n"]}],"source":["# Imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import textwrap  # Import textwrap module\n","\n","# Create the prompt\n","prompt = f\"\"\"\n","Based on the following email, provide concise answers to the questions below:\n","\n","Email:\n","```{email}```\n","\n","Questions:\n","1. What was the reason for the customer's disappointment with WidgetCorp?\n","2. How did WidgetCorp's customer service respond to the customer's inquiries about the late delivery?\n","\n","Please answer only the questions, without any additional context or the email text.\n","\"\"\"\n","\n","# Tokenize the input\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","\n","# Move inputs to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","model.to(device)\n","\n","# Generate response with specified max_new_tokens\n","with torch.no_grad():\n","    output = model.generate(**inputs, max_new_tokens=150)  # Adjust the number as needed\n","\n","# Decode the response\n","response = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","# Print the formatted response\n","answers = response.split(\"Questions:\")[-1].strip()  # Get only the answers part\n","print(textwrap.fill(answers, width=80))\n"]},{"cell_type":"markdown","metadata":{},"source":["Named Entity Recognition (NER)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:58:39.006053Z","iopub.status.busy":"2024-09-29T10:58:39.005690Z","iopub.status.idle":"2024-09-29T10:58:44.283937Z","shell.execute_reply":"2024-09-29T10:58:44.282963Z","shell.execute_reply.started":"2024-09-29T10:58:39.006018Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Identify the entities in the email below (use standard NER terminology).\n","Email:  ``` Dear WidgetCorp, I am writing to express my deep disappointment with\n","your service regarding my recent order (Order #98765). I had ordered the\n","\"SuperWidget\" as a birthday gift for my daughter, Emily, whose birthday was on\n","March 15th. Despite your website promising delivery within three days, the\n","product arrived only yesterday, missing her birthday entirely. I attempted to\n","contact your customer service multiple times for an explanation and to expedite\n","the delivery, but to no avail. My calls and emails went unanswered, leaving me\n","in a difficult position with no gift for Emily’s special day. This experience\n","has left a sour taste, and I am reconsidering future purchases from WidgetCorp.\n","I expect a prompt explanation and an assurance that such delays will not occur\n","in the future. Sincerely, Jane Doe ```   Please provide only the entities\n","without any additional text.   - WidgetCorp (Organization) - Order #98765\n","(Product) - SuperWidget (Product) - Emily (Person) - March 15th (Date) -\n","Customer Service (Organization) - Jane Doe (Person) Organization, Product,\n","Person, Date Organization, Product, Person, Date\n"]}],"source":["# Imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import textwrap  # Import textwrap module\n","\n","# Create the prompt for entity identification\n","prompt = f\"\"\"\n","Identify the entities in the email below (use standard NER terminology). \n","\n","Email: \n","```{email}``` \n","\n","Please provide only the entities without any additional text.\n","\"\"\"\n","\n","# Tokenize the input\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","\n","# Move inputs to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","model.to(device)\n","\n","# Generate response with specified max_new_tokens\n","with torch.no_grad():\n","    output = model.generate(**inputs, max_new_tokens=150)  # Adjust the number as needed\n","\n","# Decode the response\n","response = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","# Print the formatted response, focusing on entities\n","print(textwrap.fill(response.strip(), width=80))\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["English to French Translation"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:58:49.968932Z","iopub.status.busy":"2024-09-29T10:58:49.968163Z","iopub.status.idle":"2024-09-29T10:58:59.016442Z","shell.execute_reply":"2024-09-29T10:58:59.015451Z","shell.execute_reply.started":"2024-09-29T10:58:49.968894Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["``` Cher WidgetCorp,  Je vous écris pour exprimer ma profonde déception\n","concernant le service que vous avez rendu à propos de mon commande récente\n","(Numéro d'ordre #98765). J'avais commandé le \"SuperWidget\" comme cadeau\n","d'anniversaire pour ma fille, Émilie, dont le jour de naissance était le 15\n","mars. Malgré les promesses de votre site web concernant le délai de livraison de\n","trois jours, le produit est arrivé seulement hier, manquant complètement son\n","anniversaire.  J'ai essayé de cont\n"]}],"source":["# Imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import textwrap  # Import textwrap module\n","\n","# Create the prompt for translation\n","prompt = f\"\"\"\n","Translate the following email delimited by triple backticks into French.\n","\n","Email:\n","```{email}```\n","\"\"\"\n","\n","# Tokenize the input\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","\n","# Move inputs to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","model.to(device)\n","\n","# Generate response with specified max_new_tokens\n","with torch.no_grad():\n","    output = model.generate(**inputs, max_new_tokens=150)  # Adjust the number as needed\n","\n","# Decode the response\n","response = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","# Print only the translated version\n","translated_email = response.split(\"Email:\")[-1].strip()  # Extracting the translated part\n","print(textwrap.fill(translated_email, width=80))\n"]},{"cell_type":"markdown","metadata":{},"source":["French to English Translation"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:59:04.680141Z","iopub.status.busy":"2024-09-29T10:59:04.679767Z","iopub.status.idle":"2024-09-29T10:59:13.862239Z","shell.execute_reply":"2024-09-29T10:59:13.861256Z","shell.execute_reply.started":"2024-09-29T10:59:04.680093Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["``` Cher WidgetCorp,  Je vous écris pour exprimer ma profonde déception\n","concernant le service que vous avez rendu à propos de mon commande récente\n","(Numéro d'ordre #98765). J'avais commandé le \"SuperWidget\" comme cadeau\n","d'anniversaire pour ma fille, Émilie, dont le jour de naissance était le 15\n","mars. Malgré les promesses de votre site web concernant le délai de livraison de\n","trois jours, le produit est arrivé seulement hier, manquant complètement son\n","anniversaire.  J'ai essayé de cont```  Dear WidgetCorp,  I am writing to express\n","my deep disappointment with the service provided concerning my recent order\n","(Order #98765). I had ordered the \"SuperWidget\" as a birthday gift for my\n","daughter, Emily, whose birthday was on March 15th. Despite your website\n","promising delivery within three days, the product arrived only yesterday,\n","missing her birthday entirely.  I attempted to contact your customer service\n","multiple times for an explanation and to expedite the delivery, but to no avail.\n","My calls and emails went unanswered, leaving me in a difficult position without\n","a gift for Emily’s special day.  This experience has left a sour taste\n"]}],"source":["# Imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import textwrap  # Import textwrap module\n","\n","\n","# Create the prompt for translation\n","prompt = f\"\"\"\n","Translate the following email delimited by triple backticks into English.\n","\n","Email:\n","```{response}```\n","\"\"\"\n","\n","# Tokenize the input\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","\n","# Move inputs to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","model.to(device)\n","\n","# Generate response with specified max_new_tokens\n","with torch.no_grad():\n","    output = model.generate(**inputs, max_new_tokens=150)  # Adjust the number as needed\n","\n","# Decode the response\n","response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","# Extract and print only the translated email portion\n","translated_email = response_text.split(\"Email:\")[-1].strip()  # Extracting the translated part\n","print(textwrap.fill(translated_email, width=80))\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30777,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
